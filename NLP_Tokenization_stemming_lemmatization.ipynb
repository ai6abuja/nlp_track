{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Stemming, Lemmatization and Tokenization\n",
    "read blog post https://jenniferkwentoh.com/get-started-with-natural-language-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chiazor.kwentoh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the nltk package\n",
    "import nltk\n",
    "#nltk downloader\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of class PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "porter_stem = PorterStemmer()\n",
    "lancaster_stem = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Porter's Algorithm\n",
      "cat\n",
      "goat\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "#find the stem of these words \n",
    "\n",
    "print(\"Using Porter's Algorithm\")\n",
    "print(porter_stem.stem(\"cats\"))\n",
    "print(porter_stem.stem(\"goats\"))\n",
    "print(porter_stem.stem(\"running\"))\n",
    "print(porter_stem.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Lancaster's Algorithm\n",
      "zebra\n",
      "democr\n",
      "bureaucr\n",
      "perfect\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Lancaster's Algorithm\")\n",
    "\n",
    "print(lancaster_stem.stem(\"Zebras\"))\n",
    "print(lancaster_stem.stem(\"democracy\"))\n",
    "print(lancaster_stem.stem(\"bureaucracy\"))\n",
    "print(lancaster_stem.stem(\"perfection\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma  of 'quickly' is  quickly\n",
      "lemma of 'asked' is  ask\n",
      "lemma of 'cables' is  cable\n"
     ]
    }
   ],
   "source": [
    "# a denotes adjective, a word that modifies a noun\n",
    "# v denotes verb\n",
    "# n denotes noun\n",
    "  \n",
    "print(\"lemma  of 'quickly' is \", lemmatizer.lemmatize(\"quickly\", pos=\"a\")) \n",
    "\n",
    "print(\"lemma of 'asked' is \", lemmatizer.lemmatize(\"asked\", pos=\"v\"))   \n",
    "\n",
    "print(\"lemma of 'cables' is \", lemmatizer.lemmatize(\"cables\", pos =\"n\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Lazy', 'dog', 'JUMPED', 'over', 'the', 'fence']\n"
     ]
    }
   ],
   "source": [
    "text = \"The Lazy dog JUMPED over the fence\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break down into sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Lazy dog JUMPED over the fence.',\n",
       " 'The cat was chasing it.',\n",
       " 'Lagos traffic is on another level']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#break down into sentences  \n",
    "\n",
    "text = '''The Lazy dog JUMPED over the fence. \n",
    "            The cat was chasing it. \n",
    "            Lagos traffic is on another level'''\n",
    "\n",
    "tokens = sent_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex. \n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense. \n",
    "Readability counts. \n",
    "Special cases aren't special enough to break the rules. Although practicality beats purity. \n",
    "Errors should never pass silently. \n",
    "Unless explicitly silenced.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beautiful', 'is', 'better', 'than', 'ugly', '.', 'Explicit', 'is', 'better', 'than', 'implicit', '.', 'Simple', 'is', 'better', 'than', 'complex', '.', 'Complex', 'is', 'better', 'than', 'complicated', '.', 'Flat', 'is', 'better', 'than', 'nested', '.', 'Sparse', 'is', 'better', 'than', 'dense', '.', 'Readability', 'counts', '.', 'Special', 'cases', 'are', \"n't\", 'special', 'enough', 'to', 'break', 'the', 'rules', '.', 'Although', 'practicality', 'beats', 'purity', '.', 'Errors', 'should', 'never', 'pass', 'silently', '.', 'Unless', 'explicitly', 'silenced', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_Sentence(text):\n",
    "    token_words=word_tokenize(text)\n",
    "    return token_words\n",
    "\n",
    "print(tokenize_Sentence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauti is better than ugli . explicit is better than implicit . simpl is better than complex . complex is better than complic . flat is better than nest . spars is better than dens . readabl count . special case are n't special enough to break the rule . although practic beat puriti . error should never pass silent . unless explicitli silenc . \n"
     ]
    }
   ],
   "source": [
    "def porter_algorithm(tokenize_Sentence):\n",
    "    text = []\n",
    "    for word in tokenize_Sentence:\n",
    "        text.append (porter_stem.stem (word))\n",
    "        text.append(\" \")\n",
    "        stem_words = \"\".join(text)\n",
    "    return stem_words\n",
    "    \n",
    "\n",
    "print (porter_algorithm(tokenize_Sentence(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauty is bet than ug . explicit is bet than implicit . simpl is bet than complex . complex is bet than comply . flat is bet than nest . spars is bet than dens . read count . spec cas ar n't spec enough to break the rul . although pract beat pur . er should nev pass sil . unless explicit sil . \n"
     ]
    }
   ],
   "source": [
    "def lancaster_algorithm(tokenize_Sentence):\n",
    "    text = []\n",
    "    for word in tokenize_Sentence:\n",
    "        text.append (lancaster_stem.stem (word))\n",
    "        text.append(\" \")\n",
    "        stem_words = \"\".join(text)\n",
    "    return stem_words\n",
    "    \n",
    "\n",
    "print (lancaster_algorithm(tokenize_Sentence(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful is good than ugly . Explicit is good than implicit . Simple is good than complex . Complex is good than complicated . Flat is good than nested . Sparse is good than dense . Readability counts . Special cases are n't special enough to break the rules . Although practicality beats purity . Errors should never pass silently . Unless explicitly silenced . \n"
     ]
    }
   ],
   "source": [
    "def WordNet_Lemmatizer(tokenize_Sentence):\n",
    "    text = []\n",
    "    for word in tokenize_Sentence:\n",
    "        text.append (lemmatizer.lemmatize(word, pos = \"a\"))\n",
    "        text.append(\" \")\n",
    "        lemma_words = \"\".join(text)\n",
    "    return lemma_words\n",
    "    \n",
    "\n",
    "print (WordNet_Lemmatizer(tokenize_Sentence(text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
